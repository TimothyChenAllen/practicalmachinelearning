---
title: "Practical Machine Learning Course Project"
author: "Timothy Chen Allen"
date: "December 25, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

# Study
This study uses **Human Activity Recognition** data (Velloso et al, 2013), collected from six
exercisers equipped with accelerometers, to create and test a prediction function to predict
exercise quality based on data from the accelerometers.  This is a classification problem.

# Expected Out-of-Sample Error
Out-of-sample error of
the most parsimonious prediction function,
using only 18 features,
is estimated to be 0.84%, with a cross-validated
accuracy of 99.07%.

# Method
1. **Split Datasets**. Three datasets are used: the original training dataset is split into a `training` and `testing` set. The original testing dataset is held aside as the `validation` set.
2. **Define Error Rate**. Accuracy of predictions on the `training` set is used to
select covariates.  Cross-validated accuracy of predictions on the `testing` set is used to refine 
the final model, which is used one time only on the `validation` dataset.
3. **Pick Features and Cross Validate**. Initially,
53 of the 160 columns are used as features.  Features are retained
which a) are readings from the accelerometers, b) do not have large amounts of missing data, and c) do not have Near Zero Variability.  Cross-validation is done on the `testing` dataset to determine if the
features retained are useful.
4. **Pick Prediction Function**.  *Random Forest* is trained using the `training` dataset,
as it gives good accuracy on 
classification problems.  The trained prediction function is cross-validated using the `testing` data set.
5. **Seek Parsimony**.  The `training` and `testing` datasets are used to find the smallest number
of predictor features that return a good error rate.  This avoids overfitting and improves robustness.
The final prediction function is trained
and corss-validated using a set of only 18
predictor features, while retaining good estimated
out-of-sample error and accuracy.
6. **Validate**. Predictions are made using the `validation` dataset and submitted for grading.

# Expected Out-of-Sample Error
Out-of-sample error of
the most parsimonious prediction function
is estimated to be 0.84%, with a cross-validated
accuracy of 99.07%.

# Load libraries
```{r load_libraries, echo=TRUE, message=FALSE, results='hide'}
require(caret)
require(rpart)
require(dplyr)
require(randomForest)
require(e1071)
require(rpart)
```

# Split data into training, testing, and validation sets
Three datasets are used: the original training dataset is split into a `training` and `testing` set. The original testing dataset is held aside as the `validation` set.
```{r read_data, echo=TRUE, message=FALSE, results='hide'}
if (file.exists("raw_training.sav")) {
  load("raw_training.sav")
  load("validation.sav")
} else {
  raw_training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
  save(raw_training, file="raw_training.sav")
  validation <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
  save(validation, file="validation.sav")
}

# Create a training and testing set from the training data
inTrain <- createDataPartition(y=raw_training$classe, p=0.7, list=FALSE)
training <- raw_training[inTrain,]
testing <- raw_training[-inTrain,]
# Now we have a training set, a testing set, and a validation set
```

# Pick features and cross-validate (training set)
Initially, 53 of the 160 columns are used as features.  Features are retained
which a) are readings from the accelerometers, b) do not have large amounts of missing data, and 
c) do not have Near Zero Variability.  Cross-validation is done on the `testing` dataset to determine if the
features retained are useful.  Later, a more 
parsimonious set of only 18 features is used to
create and cross-validate the final prediction
function.
```{r pick_features, echo=TRUE, message=FALSE, results='hide'}
# Choose Useful Covariates
# 1) remove predictors that include NAs, and columns that are unique for the user/instance
training.important <- training[,!unlist(lapply(training, function(x) any(is.na(x))))]

# 2) use nearZeroVar function to exclude columns without sufficient variance
nz.table <- nearZeroVar(training.important, saveMetrics=TRUE)
keep.columns <- row.names(nz.table[!nz.table$nzv,])

# 3) Remove columns that identify individual records
keep.columns <- keep.columns[is.na(match(
  keep.columns,
  c(
    "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
    "cvtd_timestamp", "num_window"
  )
))]

# Retain important features in both the training and testing datasets
training.important <- training %>% select(match(keep.columns, names(training)))
#testing.important <- testing %>% #select(match(keep.columns, names(training)))
```

# Pick the prediction function and cross-validate 
*Random Forest* is trained using the `training` dataset,
as it gives good accuracy on 
classification problems.  The trained prediction function is cross-validated using the `testing` data set.
```{r pick_function, echo=TRUE, message=FALSE, results='hide'}
rf <- randomForest(classe ~ . -classe, data=training.important, ntree=200 )
```

# Apply Features and Prediction Function to Test set
```{r cross_validate, echo=TRUE, message=FALSE, results='markup'}
# predict outcome for test data set using the random forest model
#pred.important <- predict(rf,testing.important)
pred.important <- predict(rf,testing)
# compare results to testing set - Accuracy
#confusionMatrix(testing.important$classe, pred.important)
confusionMatrix(testing$classe, pred.important)
```

# Refine the Features to Seek Parsimony
Determine the most parsimonious set of features
that retains a good error rate.  Reducing the number 
of features from 53 to 18 retains a cross-validated
accuracy of 99.07% and an estimated Out-of-Sample error
of 0.84%.
```{r parsimony, echo=TRUE, message=FALSE, results='markup'}
# Important variables
importance <- varImp(rf)
important.features <- importance %>% mutate(nm=rownames(importance)) %>% arrange(desc(Overall))

training.parsimony <- training.important[,match(c("classe",important.features[1:18,]$nm),
                                                names(training.important))]

# Try a more parsimonious model based on these importance values
rf.parsimony <- randomForest(classe ~ ., data=training.parsimony, ntree=200 )
# predict outcome for test data set using the random forest model
#pred.parsimony <- predict(rf.parsimony,testing.important)
pred.parsimony <- predict(rf.parsimony,testing)

#confusionMatrix(pred.parsimony, testing.important$classe)
confusionMatrix(pred.parsimony, testing$classe)
```

# Graphical Representation of Parsimonious Features
The following graphic gives some idea of how `classe`
is grouped by the first two features used to
train the prediction function.  In reality,
18 features were used to train the random forest.
```{r parsimony_graphic, echo=FALSE}
qplot(roll_belt, yaw_belt, col=classe, data=training)
```

# Apply Prediction Function to Validation Dataset
Apply refined features and prediction function to validation set.  
Out-of-sample error of
the most parsimonious prediction function
is estimated to be 0.84%.  In practice,
20 predictions, based on the `validation` dataset, 
were uploaded for grading and found to be 100% accurate.

```{r validation_set, echo=TRUE, message=FALSE, results='markup'}
pred.validation <- predict(rf.parsimony,validation)
```

# References
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. **Qualitative Activity Recognition of Weight Lifting Exercises**. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har